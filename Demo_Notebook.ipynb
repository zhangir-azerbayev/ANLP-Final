{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c28c8d46",
   "metadata": {},
   "source": [
    "# Curriculum Learning Final Project\n",
    "Note: this notebook is not meant to be run, only to display the code. If you wish to replicate my results, run the scripts provided in the same directory (most importantly `train.py`, `train_curriculum.py`, and `evaluate.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3db17a",
   "metadata": {},
   "source": [
    "## Program Execution\n",
    "Below we define a somewhat safe environment for executing generated code. We do not fully sandbox execution, but disable most harmful OS capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923f0f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically copied from https://github.com/openai/human-eval/blob/master/human_eval/execution.py\n",
    "\n",
    "from typing import Optional, Callable, Dict\n",
    "import ast\n",
    "import contextlib\n",
    "import faulthandler\n",
    "import io\n",
    "import os\n",
    "import multiprocessing\n",
    "import platform\n",
    "import signal\n",
    "import tempfile\n",
    "\n",
    "\n",
    "def semisafe_evaluate(completion: str, key : str, timeout : float):\n",
    "    \"\"\"\n",
    "    Evaluates a code completion with some added safety features. \n",
    "    Returns a float if code succesfully executes. Returns a string describing \n",
    "    error if code is unsuccesfull. \n",
    "    \"\"\"\n",
    "\n",
    "    def semisafe_execute():\n",
    "\n",
    "        with create_tempdir():\n",
    "\n",
    "            # These system calls are needed when cleaning up tempdir.\n",
    "            import os\n",
    "            import shutil\n",
    "            rmtree = shutil.rmtree\n",
    "            rmdir = os.rmdir\n",
    "            chdir = os.chdir\n",
    "\n",
    "            # Disable functionalities that can make destructive changes to the test.\n",
    "            reliability_guard()\n",
    "\n",
    "            # Construct the check program and run it.\n",
    "\n",
    "            try:\n",
    "                exec_globals = {}\n",
    "                with swallow_io():\n",
    "                    with time_limit(timeout):\n",
    "                         exec(completion, exec_globals)\n",
    "                if key in exec_globals.keys(): \n",
    "                    result.append(exec_globals[key])\n",
    "                else: \n",
    "                    result.append(\"answer not computed\")\n",
    "            except TimeoutException:\n",
    "                result.append(\"timed out\")\n",
    "            except BaseException as e:\n",
    "                result.append(f\"failed: {e}\")\n",
    "\n",
    "            # Needed for cleaning up.\n",
    "            shutil.rmtree = rmtree\n",
    "            os.rmdir = rmdir\n",
    "            os.chdir = chdir\n",
    "\n",
    "    manager = multiprocessing.Manager()\n",
    "    result = manager.list()\n",
    "\n",
    "    p = multiprocessing.Process(target=semisafe_execute)\n",
    "    p.start()\n",
    "    p.join(timeout=timeout + 1)\n",
    "    if p.is_alive():\n",
    "        p.kill()\n",
    "\n",
    "    if not result:\n",
    "        result.append(\"timed out\")\n",
    "\n",
    "    return result[0]\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def time_limit(seconds: float):\n",
    "    def signal_handler(signum, frame):\n",
    "        raise TimeoutException(\"Timed out!\")\n",
    "    signal.setitimer(signal.ITIMER_REAL, seconds)\n",
    "    signal.signal(signal.SIGALRM, signal_handler)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        signal.setitimer(signal.ITIMER_REAL, 0)\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def swallow_io():\n",
    "    stream = WriteOnlyStringIO()\n",
    "    with contextlib.redirect_stdout(stream):\n",
    "        with contextlib.redirect_stderr(stream):\n",
    "            with redirect_stdin(stream):\n",
    "                yield\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def create_tempdir():\n",
    "    with tempfile.TemporaryDirectory() as dirname:\n",
    "        with chdir(dirname):\n",
    "            yield dirname\n",
    "\n",
    "\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class WriteOnlyStringIO(io.StringIO):\n",
    "    \"\"\" StringIO that throws an exception when it's read from \"\"\"\n",
    "\n",
    "    def read(self, *args, **kwargs):\n",
    "        raise IOError\n",
    "\n",
    "    def readline(self, *args, **kwargs):\n",
    "        raise IOError\n",
    "\n",
    "    def readlines(self, *args, **kwargs):\n",
    "        raise IOError\n",
    "\n",
    "    def readable(self, *args, **kwargs):\n",
    "        \"\"\" Returns True if the IO object can be read. \"\"\"\n",
    "        return False\n",
    "\n",
    "\n",
    "class redirect_stdin(contextlib._RedirectStream):  # type: ignore\n",
    "    _stream = 'stdin'\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def chdir(root):\n",
    "    if root == \".\":\n",
    "        yield\n",
    "        return\n",
    "    cwd = os.getcwd()\n",
    "    os.chdir(root)\n",
    "    try:\n",
    "        yield\n",
    "    except BaseException as exc:\n",
    "        raise exc\n",
    "    finally:\n",
    "        os.chdir(cwd)\n",
    "\n",
    "\n",
    "def reliability_guard(maximum_memory_bytes: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    This disables various destructive functions and prevents the generated code\n",
    "    from interfering with the test (e.g. fork bomb, killing other processes,\n",
    "    removing filesystem files, etc.)\n",
    "    WARNING\n",
    "    This function is NOT a security sandbox. Untrusted code, including, model-\n",
    "    generated code, should not be blindly executed outside of one. See the \n",
    "    Codex paper for more information about OpenAI's code sandbox, and proceed\n",
    "    with caution.\n",
    "    \"\"\"\n",
    "\n",
    "    if maximum_memory_bytes is not None:\n",
    "        import resource\n",
    "        resource.setrlimit(resource.RLIMIT_AS, (maximum_memory_bytes, maximum_memory_bytes))\n",
    "        resource.setrlimit(resource.RLIMIT_DATA, (maximum_memory_bytes, maximum_memory_bytes))\n",
    "        if not platform.uname().system == 'Darwin':\n",
    "            resource.setrlimit(resource.RLIMIT_STACK, (maximum_memory_bytes, maximum_memory_bytes))\n",
    "\n",
    "    faulthandler.disable()\n",
    "\n",
    "    import builtins\n",
    "    builtins.exit = None\n",
    "    builtins.quit = None\n",
    "\n",
    "    import os\n",
    "    os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "    os.kill = None\n",
    "    os.system = None\n",
    "    os.putenv = None\n",
    "    os.remove = None\n",
    "    os.removedirs = None\n",
    "    os.rmdir = None\n",
    "    os.fchdir = None\n",
    "    os.setuid = None\n",
    "    os.fork = None\n",
    "    os.forkpty = None\n",
    "    os.killpg = None\n",
    "    os.rename = None\n",
    "    os.renames = None\n",
    "    os.truncate = None\n",
    "    os.replace = None\n",
    "    os.unlink = None\n",
    "    os.fchmod = None\n",
    "    os.fchown = None\n",
    "    os.chmod = None\n",
    "    os.chown = None\n",
    "    os.chroot = None\n",
    "    os.fchdir = None\n",
    "    os.lchflags = None\n",
    "    os.lchmod = None\n",
    "    os.lchown = None\n",
    "    os.getcwd = None\n",
    "    os.chdir = None\n",
    "\n",
    "    import shutil\n",
    "    shutil.rmtree = None\n",
    "    shutil.move = None\n",
    "    shutil.chown = None\n",
    "\n",
    "    import subprocess\n",
    "    subprocess.Popen = None  # type: ignore\n",
    "    \n",
    "    # Next line causes an error for some reason. \n",
    "    #__builtins__['help'] = None\n",
    "\n",
    "    import sys\n",
    "    sys.modules['ipdb'] = None\n",
    "    sys.modules['joblib'] = None\n",
    "    sys.modules['resource'] = None\n",
    "    sys.modules['psutil'] = None\n",
    "    sys.modules['tkinter'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe8b5bd",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We use the following function and class to parse the MathQAPython jsons and to retrieve tokenized instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab6b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import json \n",
    "from pathlib import Path \n",
    "\n",
    "def read_mathqapython(path): \n",
    "    path = Path(path)\n",
    "    with open(path, 'rb') as f: \n",
    "        mathqapython_list = json.load(f)\n",
    "\n",
    "\n",
    "    return mathqapython_list\n",
    "\n",
    "class MathQAPython(torch.utils.data.Dataset): \n",
    "    def __init__(self, sorted_instances, tokenizer, max_length): \n",
    "        self.data = sorted_instances\n",
    "        self.tokenizer = tokenizer \n",
    "        self.max_length = max_length\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        instance = self.data[idx]\n",
    "        text = instance['text'] \n",
    "        solution = instance['text'] + '\\n' + instance['code'] \n",
    "        answer = instance['answer']\n",
    "\n",
    "        text_encode = self.tokenizer(text, \n",
    "                max_length=self.max_length, truncation=True, \n",
    "                padding='max_length', return_tensors='pt')\n",
    "        solution_encode = self.tokenizer(solution, \n",
    "                max_length=self.max_length, truncation=True, \n",
    "                padding='max_length', return_tensors='pt')\n",
    "        text_ids = text_encode['input_ids'].squeeze()\n",
    "        solution_ids = solution_encode['input_ids'].squeeze()\n",
    "        solution_attn = solution_encode['attention_mask'].squeeze()\n",
    "\n",
    "        return text_ids.long(), solution_ids.long(), solution_attn.long(), answer\n",
    "\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.data) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd43fae8",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "Our baseline model is straightforward: we simply use the default Huggingface infrastructure for fine-tuning autoregressive language models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ca9949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import torch \n",
    "from torch.utils.data import RandomSampler, BatchSampler \n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "from transformers import TrainingArguments, Trainer \n",
    "\n",
    "from dataset import read_mathqapython, MathQAPython\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "# Tokenizer \n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "max_length = 450\n",
    "\n",
    "# Loading data\n",
    "print(\"loading data\")\n",
    "data = read_mathqapython('data/mathqapython_train.json')\n",
    "train_set = MathQAPython(data, tokenizer, max_length)\n",
    "\n",
    "# model \n",
    "print('loading model')\n",
    "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"./train_results/default\",\n",
    "                                  num_train_epochs=5,\n",
    "                                  per_device_train_batch_size=8,\n",
    "                                  logging_strategy=\"epoch\",\n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  weight_decay=0.01,\n",
    "                                  warmup_steps = 100,\n",
    "                                  logging_dir=\"./train_results/default/log\"\n",
    "                                  )\n",
    "\n",
    "def data_collator(data):\n",
    "    return {'input_ids': torch.stack([f[1] for f in data]),\n",
    "            'attention_mask': torch.stack([f[2] for f in data]),\n",
    "            'labels': torch.stack([f[1] for f in data])\n",
    "           }\n",
    "\n",
    "Trainer(model=model, args=training_args, train_dataset=train_set,\n",
    "        data_collator=data_collator).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac13b2e",
   "metadata": {},
   "source": [
    "## Curriculum Learning\n",
    "We implement curriculum learning by defining a `CurriculumSampler` that subclasses the PyTorch `Sampler`. The `CurriculumSampler` presupposes that the dataset is sorted according to the curriculum learning scoring function. Then, the `CurriculumSampler`'s job is to tell the dataloader which dataset indices each minibatch should consist of. \n",
    "\n",
    "To sample minibatches, we use the same algorithm found in (Hacohen and Weinshall, 2019) (see writeup bibliography for reference). Namely, to form the `i`th minibatch, we sample from the `pacer(i)` easiest instances in the dataset. \n",
    "\n",
    "Note the definition of our pacing function is as described in the writeup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a67e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import random \n",
    "from torch.utils.data import Sampler \n",
    "\n",
    "def pacer(itr, n, step_length, increase, start_prop): \n",
    "    return round(n * min(1, start_prop * math.pow(increase, math.floor(itr / step_length))))\n",
    "\n",
    "def get_instance(num_instances): \n",
    "    while True: \n",
    "        idxs = list(range(num_instances))\n",
    "        random.shuffle(idxs)\n",
    "        for idx in idxs: \n",
    "            yield idx\n",
    "\n",
    "\n",
    "class CurriculumSampler(Sampler): \n",
    "    def __init__(self, num_iters, iters_per_refresh, increase, start_prop, \n",
    "            batch_size, n): \n",
    "        self.num_iters = num_iters\n",
    "        self.iters_per_refresh = iters_per_refresh\n",
    "        self.increase = increase\n",
    "        self.start_prop = start_prop\n",
    "        self.batch_size = batch_size \n",
    "        self.n = n \n",
    "\n",
    "    def __iter__(self): \n",
    "        num_iters = self.num_iters\n",
    "        iters_per_refresh = self.iters_per_refresh\n",
    "        increase = self.increase \n",
    "        start_prop = self.start_prop\n",
    "        batch_size = self.batch_size\n",
    "        n = self.n \n",
    "\n",
    "        for itr in range(num_iters): \n",
    "            if itr % iters_per_refresh == 0:\n",
    "                # Do refresh \n",
    "                num_instances = pacer(itr, n, iters_per_refresh, increase, start_prop)\n",
    "                instance_gen = get_instance(num_instances)\n",
    "            batch = [next(instance_gen) for _ in range(batch_size)]\n",
    "            yield batch \n",
    "\n",
    "    def __len__(self): \n",
    "        return self.num_iters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1a59f4",
   "metadata": {},
   "source": [
    "After defining the `CurriculumSampler`, writing our training loop becomes straightforward. The only differences from the baseline is that we must sort our dataset according to the curriculum scoring function and that subclass the Huggingface `Trainer` in order to construct a Pytorch `Dataloader` using the `CurriculumSampler` instead of the default `Sampler`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9621b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import random \n",
    "import math \n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "from transformers import TrainingArguments, Trainer \n",
    "\n",
    "from dataset import read_mathqapython, MathQAPython\n",
    "\n",
    "from curriculum import CurriculumSampler\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Tokenizer \n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "max_length = 450\n",
    "# Loading data\n",
    "print(\"loading data\")\n",
    "data = read_mathqapython('data/mathqapython_train.json')\n",
    "\n",
    "def seqlen(instance): \n",
    "    return len(tokenizer.encode(instance['text']+instance['code']))\n",
    "\n",
    "sorted_data = sorted(data, key=seqlen)\n",
    "train_set = MathQAPython(sorted_data, tokenizer, max_length)\n",
    "\n",
    "# model \n",
    "print('loading model')\n",
    "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "\n",
    "# curriculum learning stuff \n",
    "\n",
    "\n",
    "\n",
    "class CurriculumTrainer(Trainer): \n",
    "    def get_train_dataloader(self) -> DataLoader: \n",
    "        sampler = CurriculumSampler(12000, \n",
    "                                    1000, \n",
    "                                    1.4, \n",
    "                                    0.1, \n",
    "                                    8, \n",
    "                                    len(data)\n",
    "                                    )\n",
    "\n",
    "        return DataLoader(\n",
    "                self.train_dataset, \n",
    "                collate_fn = self.data_collator, \n",
    "                num_workers = self.args.dataloader_num_workers, \n",
    "                pin_memory = self.args.dataloader_pin_memory, \n",
    "                batch_sampler = sampler\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "training_args = TrainingArguments(output_dir=\"./train_results/curriculum2\",\n",
    "                                  num_train_epochs=1,\n",
    "                                  logging_steps = 500, \n",
    "                                  save_steps = 500,\n",
    "                                  weight_decay=0.01,\n",
    "                                  warmup_steps = 100,\n",
    "                                  logging_dir=\"./train_results/curriculum2/log\"\n",
    "                                  )\n",
    "\n",
    "def data_collator(data):\n",
    "    return {'input_ids': torch.stack([f[1] for f in data]),\n",
    "            'attention_mask': torch.stack([f[2] for f in data]),\n",
    "            'labels': torch.stack([f[1] for f in data])\n",
    "           }\n",
    "\n",
    "CurriculumTrainer(model=model, args=training_args, train_dataset=train_set,\n",
    "        data_collator=data_collator).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a872c9",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeb622a",
   "metadata": {},
   "source": [
    "Our evaluation script is a standard-one for the text-to-code task that evaluates pass@1 and pass@10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2bfb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import random\n",
    "import yaml\n",
    "import json\n",
    "import os\n",
    "import itertools\n",
    "from tqdm import tqdm \n",
    "random.seed(1)\n",
    "\n",
    "import numpy as np \n",
    "np.random.seed(1)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "torch.manual_seed(1)\n",
    "\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "from dataset import read_mathqapython\n",
    "\n",
    "from execution import semisafe_evaluate\n",
    "\n",
    "device = 'cuda:1'\n",
    "\n",
    "model_path = sys.argv[1]\n",
    "outfile = sys.argv[2]\n",
    "\n",
    "# Loads model and data \n",
    "print('loading model and data...')\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "# Test set\n",
    "data = read_mathqapython('data/mathqapython_test.json')\n",
    "random.shuffle(data)\n",
    "data = data[:1000]\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def pass_k(lst, k): \n",
    "    \"\"\"\n",
    "    lst: Boolean list \n",
    "    k: value of pass@k to calculate. \n",
    "    \"\"\"\n",
    "    n = len(lst)\n",
    "    c = sum(lst)\n",
    "    if n - c < k: return 1.0 \n",
    "    return 1.0 - np.prod(1.0 - k / \n",
    "                        np.arange(n-c+1, n+1))\n",
    "\n",
    "\n",
    "results = []\n",
    "for instance in tqdm(data): \n",
    "    label = instance['answer']\n",
    "    encoded_prompt = tokenizer.encode(instance['text'], \n",
    "            return_tensors='pt').to(device)\n",
    "    prompt_length = torch.numel(encoded_prompt)\n",
    "    with torch.no_grad(): \n",
    "        out = model.generate(\n",
    "                input_ids=encoded_prompt,\n",
    "                do_sample=True ,\n",
    "                temperature=0.2, \n",
    "                max_length = min([prompt_length+250, 450]), \n",
    "                pad_token_id=tokenizer.eos_token_id, \n",
    "                num_return_sequences=10\n",
    "                )\n",
    "        generated_ids = [ids[prompt_length:] for ids in out]\n",
    "\n",
    "    untrunced_bodies = [tokenizer.decode(sample, skip_special_tokens=True)\n",
    "            for sample in generated_ids]\n",
    "    \n",
    "    re_key = '^answer.*?\\n'\n",
    "    bodies = [completion[:re.search(re_key, completion).span()[1]]\n",
    "            if re.search(re_key, completion) else completion \n",
    "            for completion in untrunced_bodies]\n",
    "\n",
    "    answers = [semisafe_evaluate(program, 'answer', 1) for program in bodies]\n",
    "\n",
    "    passed_lst = [(abs((answer - label)/label) < 0.01)\n",
    "            if isinstance(answer, float) else False for answer in answers]\n",
    "\n",
    "    result = dict()\n",
    "\n",
    "    result[10] = pass_k(passed_lst, 10)\n",
    "    result[1] = pass_k(passed_lst, 1)\n",
    "\n",
    "    if True in passed_lst: \n",
    "        best_completion = bodies[passed_lst.index(True)]\n",
    "    else: \n",
    "        best_completion = bodies[0]\n",
    "\n",
    "    result['best_completion'] = best_completion \n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "pass10scores = [instance[10] for instance in results]\n",
    "pass10average = sum(pass10scores)/len(pass10scores)\n",
    "print(\"pass 10: \", pass10average)\n",
    "\n",
    "pass1scores = [instance[1] for instance in results]\n",
    "pass1average = sum(pass1scores)/len(pass1scores)\n",
    "print(\"pass 1: \", pass1average)\n",
    "\n",
    "to_dump = {'pass1': pass1average, \n",
    "        'pass10': pass10average, \n",
    "        'results': results}\n",
    "\n",
    "with open(outfile, 'w') as fle: \n",
    "    json.dump(to_dump, fle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
